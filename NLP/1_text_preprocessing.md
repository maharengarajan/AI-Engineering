# üî• NLP Preprocessing Techniques (Core Concepts)

Preprocessing helps convert raw text into a clean, structured form so models can understand it better.

---

# 1Ô∏è‚É£ **Tokenization**

**Breaking text into smaller units** ‚Üí words, subwords, or sentences.

Example:
`"I love Natural Language Processing"` ‚Üí
`["I", "love", "Natural", "Language", "Processing"]`

Types:

* **Word tokenization**
* **Sentence tokenization**
* **Subword tokenization** (BPE, WordPiece ‚Üí used in BERT, GPT etc.)

---

# 2Ô∏è‚É£ **Lowercasing (optional)**

Convert text to lowercase.
`"Apple"` ‚Üí `"apple"`

‚úîÔ∏è Good for classical ML
‚ùå Not used in some transformer models (e.g., BERT has cased & uncased versions)

---

# 3Ô∏è‚É£ **Stopword Removal**

Stopwords are **common words that do not add much meaning**.

Examples::
`is`, `am`, `the`, `and`, `was`, `of`

Sentence:
`"I am learning NLP"` ‚Üí remove stopwords ‚Üí `"learning NLP"`

Why remove?

* Reduces noise
* Reduces dimensionality in Bag-of-Words, TF-IDF

But **not always recommended** for transformers (they understand context).

---

# 4Ô∏è‚É£ **Stemming**

Stemming chops words to their base/root form by **rule-based truncation**.

Examples:

* `"playing"` ‚Üí `play`
* `"studies"` ‚Üí `studi`
* `"better"` ‚Üí `better` (irregular words fail)

Popular stemmers:

* Porter Stemmer
* Snowball Stemmer
* Lancaster Stemmer

‚úîÔ∏è Simple, fast
‚ùå Not linguistically correct (may distort words)

---

# 5Ô∏è‚É£ **Lemmatization**

Lemmatization converts words to their **dictionary root form (lemma)** using grammar rules.

Examples:

* `"studies"` ‚Üí `study`
* `"better"` ‚Üí `good`
* `"running"` ‚Üí `run`

It uses:

* Part-of-speech (POS)
* WordNet dictionary

‚úîÔ∏è More accurate than stemming
‚ùå Slower

---

# 6Ô∏è‚É£ **Part-of-Speech (POS) Tagging**

Assigning grammatical roles:

Example:
`"Time flies like an arrow"`

* Time ‚Üí noun
* flies ‚Üí verb
* like ‚Üí preposition

Useful for:

* Lemmatization
* Named Entity Recognition
* Parsing

---

# 7Ô∏è‚É£ **Removing Punctuation**

`"Hello!!! How are you?"` ‚Üí `"Hello How are you"`

Useful for:

* Classical ML
* Sentiment analysis

Not always needed for transformers.

---

# 8Ô∏è‚É£ **Removing Numbers**

`"I bought 5 apples"` ‚Üí `"I bought apples"`

Depends on the task.
(For finance, numbers are important ‚Üí don‚Äôt remove)

---

# 9Ô∏è‚É£ **Normalizing Contractions**

Convert short forms to full forms:

* `don't` ‚Üí `do not`
* `I'm` ‚Üí `I am`

Improves clarity for models.

---

# üîü **Handling Emojis / Emoticons**

Emojis give emotional cues.
üòä ‚Üí "smiling face"
üò° ‚Üí "angry face"

Useful for sentiment analysis.

---

# 1Ô∏è‚É£1Ô∏è‚É£ **Text Normalization**

Includes:

* Replacing repeated characters ‚Üí `"soooo good"` ‚Üí `"so good"`
* Fixing spelling ‚Üí `"teh"` ‚Üí `"the"`

---

# 1Ô∏è‚É£2Ô∏è‚É£ **Vectorization (Feature Extraction)**

Converting text to numbers:

* **Bag of Words**
* **TF-IDF**
* **Word2Vec**
* **GloVe**
* **BERT embeddings**

---

# üåü Stemming vs Lemmatization (Very Important for Interviews)

| Feature  | Stemming            | Lemmatization           |
| -------- | ------------------- | ----------------------- |
| Output   | Root by chopping    | Dictionary root (lemma) |
| Grammar  | Ignores grammar     | Grammar-aware           |
| Speed    | Fast                | Slower                  |
| Accuracy | Low                 | High                    |
| Example  | studies ‚Üí **studi** | studies ‚Üí **study**     |

---

# ‚úîÔ∏è When to Use Which?

### **For classical ML models (TF-IDF, Bag-of-Words):**

* Stopwords removal ‚Üí YES
* Stemming/Lemmatization ‚Üí YES

### **For transformer models (BERT, GPT):**

* Stopwords removal ‚Üí NO (they understand context)
* Stemming/Lemmatization ‚Üí NO (destroys context)
* Tokenization ‚Üí Done by model tokenizer (WordPiece, BPE)

---
