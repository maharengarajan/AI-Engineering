# ‚úÖ **What is PyTorch?**

**PyTorch** is an **open-source deep learning framework** developed by **Facebook (Meta)**.
It is used to build and train neural networks for AI/ML tasks like:

* Computer vision
* NLP
* Generative AI
* Reinforcement learning
* Recommendation systems
* LLMs

Think of PyTorch as a **Python-friendly toolkit** that lets you:

* Create neural networks
* Run them on GPU
* Easily experiment with ideas
* Debug like normal Python code

---

# ‚úÖ **Why PyTorch Became So Popular**

### **1. Pythonic and Easy to Use**

PyTorch feels like writing **regular Python**, unlike older frameworks that felt rigid.

```python
x = torch.tensor([1.0], requires_grad=True)
y = x * 5
y.backward()
print(x.grad)
```

This simple, intuitive design attracted researchers and developers.

---

### **2. Dynamic Computation Graph ("define-by-run")**

PyTorch builds the computation graph **on the fly** as code runs.

This means:

‚úî Easy debugging
‚úî Flexible model building
‚úî Works well with loops, conditions, custom architectures

This is a big reason researchers switched from TensorFlow 1.x.

---

### **3. Strong GPU & Automatic Differentiation Support**

PyTorch has:

* Fast GPU operations
* Autograd engine for computing gradients automatically
* Simple API:

```python
tensor.cuda()
```

This made training deep learning models extremely convenient.

---

### **4. Massive Adoption in Research**

Most new research papers used PyTorch because it was fast to experiment with.
As research moved to PyTorch ‚Üí industry followed.

---

### **5. Backed by Meta + Huge Community**

* Large community
* Tons of tutorials
* Supported by major AI labs
* Used in many production systems (Meta, Tesla, OpenAI earlier, etc.)

---

### **6. Ecosystem Tools (Huge Advantage)**

PyTorch has powerful extensions:

| Library                      | Purpose                 |
| ---------------------------- | ----------------------- |
| **TorchVision**              | Computer vision         |
| **TorchText**                | NLP                     |
| **TorchAudio**               | Audio tasks             |
| **TorchServe**               | Model deployment        |
| **PyTorch Lightning**        | Simplified training     |
| **HuggingFace Transformers** | Most models use PyTorch |

This ecosystem made it the default choice for ML engineers.

---

### **7. Stability + Production Ready**

PyTorch evolved from a research tool to a production framework.

* **TorchScript** for optimization
* **Mobile support**
* **ONNX export**
* **Distributed training**

Companies trust it for scalable model training.

---

# ‚≠ê **Summary**

**PyTorch = Easy + Flexible + Powerful GPU Training + Huge Community**
That‚Äôs why it became the dominant deep learning framework.

---
# ‚úÖ **Simplest, cleanest example** of creating and training a basic neural network using **PyTorch**.

This example trains a small network to learn the function:
**y = 2x + 1** (just a simple regression toy example)

---

# ‚úÖ **1. Install PyTorch (if not installed)**

```bash
pip install torch
```

---

# ‚úÖ **2. Simple Neural Network in PyTorch**

### üìå **Full Working Code**

```python
import torch
import torch.nn as nn
import torch.optim as optim

# --------------------------------------
# 1. Create Dummy Dataset
# --------------------------------------
# y = 2x + 1
X = torch.randn(100, 1)    # 100 samples, 1 feature
y = 2 * X + 1

# --------------------------------------
# 2. Define Neural Network
# --------------------------------------
class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.layer1 = nn.Linear(1, 16)
        self.layer2 = nn.Linear(16, 1)

    def forward(self, x):
        x = torch.relu(self.layer1(x))
        x = self.layer2(x)
        return x

model = SimpleNN()

# --------------------------------------
# 3. Loss Function & Optimizer
# --------------------------------------
criterion = nn.MSELoss()         # Mean Squared Error for regression
optimizer = optim.Adam(model.parameters(), lr=0.01)

# --------------------------------------
# 4. Training Loop
# --------------------------------------
epochs = 200

for epoch in range(epochs):
    # Forward pass
    predictions = model(X)
    loss = criterion(predictions, y)

    # Backward pass
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    # Print every 20 epochs
    if (epoch + 1) % 20 == 0:
        print(f"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}")

# --------------------------------------
# 5. Test the Model
# --------------------------------------
test_input = torch.tensor([[5.0]])
predicted = model(test_input)

print("\nTest Input: 5")
print("Predicted Output:", predicted.item())
```

---

# ‚úÖ **What is a Tensor?**

A **tensor = a container for numbers** arranged in a specific shape.

### You already know tensors (even without realizing):

| Tensor Type   | Meaning                                             | Example                 |
| ------------- | --------------------------------------------------- | ----------------------- |
| **0D tensor** | Scalar (single number)                              | `5`                     |
| **1D tensor** | Vector                                              | `[3, 5, 7]`             |
| **2D tensor** | Matrix                                              | `[[1,2],[3,4]]`         |
| **3D tensor** | Stack of matrices                                   | 10 images of size 28√ó28 |
| **4D tensor** | Batch of images (batch √ó channels √ó height √ó width) | `(32, 3, 64, 64)`       |
| **nD tensor** | Higher-dimensional data                             | Anything beyond 4D      |

So **tensors generalize scalars, vectors, and matrices**.

---

# ‚úÖ **Why are Tensors Important in Deep Learning?**

### ‚úî They store data (inputs, weights, outputs).

### ‚úî They support fast GPU computation.

### ‚úî They allow automatic differentiation for backpropagation.

Every neural network operation (matmul, convolution, activation) uses tensors.

---

# üß† Why not use NumPy arrays?

Because tensors can:

‚úî Run on **GPU**
‚úî Support **autograd** for calculating gradients
‚úî Integrate with neural network layers

NumPy cannot do these directly.

---

# ‚≠ê Summary

**Tensors = fundamental building blocks of deep learning.**
They‚Äôre just **multidimensional arrays** with GPU + autograd support.

---