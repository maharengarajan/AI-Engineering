# âœ… **1. BLEU Score (Bilingual Evaluation Understudy)**

**Used for:** Machine Translation, Text Generation
**Type:** Precision-based metric (how much of your generated text appears in the reference)

### **Idea**

BLEU measures **how similar the generated text is to a reference** by counting overlapping **n-grams** (like 1-gram, 2-gram).

### ğŸ¯ Key Points

* Measures **precision** â†’ â€œHow much of what I generated is correct?â€
* Penalty for **too short** output (brevity penalty)
* Range: **0 to 1** (or 0 to 100%)
* Higher = better

### ğŸ”¹ Example

Generated:
`I love eating apples`

Reference:
`I like eating apples`

BLEU will count overlap:

* â€œIâ€, â€œeatingâ€, â€œapplesâ€ match
* â€œloveâ€ doesnâ€™t match â€œlikeâ€

---

# âœ… **2. ROUGE Score (Recall-Oriented Understudy for Gisting Evaluation)**

**Used for:** Summarization, Similarity
**Type:** Recall-based metric (how much of the reference appears in your generated text)

### **Idea**

ROUGE checks **how much of the reference summary is captured by your modelâ€™s summary**.

### ğŸ¯ Types

* **ROUGE-1** â†’ unigram overlap
* **ROUGE-2** â†’ bigram overlap
* **ROUGE-L** â†’ longest common subsequence (sequence similarity)

### ğŸ”¹ Example

Reference summary contains important words.
ROUGE checks: *Does your modelâ€™s summary include them?*

### âœ” Why used in summarization?

Because human-written summary = â€œground truth important info.â€
We want to see if our summary contains the **important points** (recall).

---

# âœ… **3. Perplexity**

**Used for:** Language Models (LMs), text generation quality
**Type:** Probability-based metric

### **Idea**

Perplexity measures **how well the model predicts the next word**.

Lower perplexity = better language model.

### ğŸ¯ Interpretation

If perplexity = 20
â†’ model is as â€œconfusedâ€ as if it had to choose between 20 words.

### âœ” Why useful?

* Evaluates LLMs, GPT-style models
* Not dependent on reference text
* Based on modelâ€™s internal probability distribution

---

# âœ… **4. Accuracy / F1 Score**

**Used for:** Classification NLP tasks

* Text classification
* Sentiment analysis
* NER (Named Entity Recognition)
* POS tagging

### ğŸŸ¦ NER example

Predicted:
`Apple` â†’ ORG
Actual:
`Apple` â†’ ORG
Good!

But if the span or label is off, metrics drop.

**F1 Score** combines

* Precision (what you predicted correctly)
* Recall (what you should have predicted)

---

# âœ… **5. METEOR Score**

**Used for:** Translation, Paraphrasing
**Better than BLEU** because:

* Includes **stemming**
* Includes **synonyms**
* Considers **word order**

### Example

Generated: â€œHe purchased a carâ€
Reference: â€œHe bought a carâ€

BLEU: low (bought â‰  purchased)
METEOR: high (purchased â‰ˆ bought)

---

# âœ… **6. CIDEr Score**

**Used for:** Image Captioning
Measures consensus between generated captions and multiple human captions.

Focuses on important words using TF-IDF weighting.

---

# âœ… **7. BERTScore**

**Used for:** Translation, Summarization, Paraphrasing
**Semantic similarity metric**

Uses BERT embeddings â†’ checks meaning, not just matching words.

### Example

Generated: "He went to the store"
Reference: "He visited the shop"

BLEU â†’ low
BERTScore â†’ high (because meaning matches)

---

# ğŸ“Œ **Which Metric Should You Use?**

| Task Type                             | Best Metrics                         |
| ------------------------------------- | ------------------------------------ |
| **Machine Translation**               | BLEU, METEOR, BERTScore              |
| **Summarization**                     | ROUGE-1, ROUGE-2, ROUGE-L, BERTScore |
| **Language Models**                   | Perplexity                           |
| **Text Generation (Chat/LLM)**        | BERTScore, Human Evaluation          |
| **Image Captioning**                  | CIDEr, BLEU                          |
| **Classification (Sentiment, Topic)** | Accuracy, Precision, Recall, F1      |
| **NER**                               | F1 Score (strict or relaxed)         |

---

# ğŸ”¥ Quick Memory Trick

* **BLEU â†’ Precision â†’ Translation**
* **ROUGE â†’ Recall â†’ Summarization**
* **Perplexity â†’ How confused is the LM?**
* **BERTScore â†’ Semantic similarity**
* **F1 â†’ For classification/NER (balanced metric)**

---
