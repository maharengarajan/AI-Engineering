# 1ï¸âƒ£ What is a *context window*

ðŸ‘‰ **Context window = the maximum amount of text the model can â€œseeâ€ at one time**

That includes:

* what **you send** (system + user messages)
* what the **model has already replied**
* what the **model is about to generate**

Once the window is full, the model **cannot see anything beyond it**.

Think of it like a **whiteboard**:

* You keep writing on it
* When itâ€™s full, you must **erase something** before adding more

---

# 2ï¸âƒ£ What counts toward the context window?

Everything below counts as tokens:

```
System prompt
+ Conversation history
+ Documents you paste
+ Tool results
+ Modelâ€™s previous replies
+ Modelâ€™s new reply (being generated)
-----------------------------------
= Total tokens in context window
```

ðŸ“Œ **Important:**
The model must *reserve space* for its answer **inside the same window**.

---

# 3ï¸âƒ£ Example: 200K context window (Claude Sonnet 4.5 standard)

Letâ€™s say the model has:

* Context window: **200,000 tokens**

You send:

* 150K tokens of documents
* 20K tokens of chat history

So far:

```
150K + 20K = 170K used
```

Now the model wants to answer.

That answer must **fit in the remaining space**:

```
200K âˆ’ 170K = 30K max output
```

If you ask for a 50K-token answer â†’ âŒ **error**
Because the model has nowhere to put it.

---

# 4ï¸âƒ£ Why â€œoutput limitâ€ exists

Even if the window is 200K, models often have a **practical output cap**.

For example:

* Claude Sonnet 4.5 may support:

  * 200K total context
  * ~128K max output in one response

So this is valid:

```
Input: 70K
Output: 128K
Total: 198K
```

But this is NOT:

```
Input: 120K
Output: 128K
Total: 248K âŒ
```

---

# 5ï¸âƒ£ 1M context window (Extended / Beta)

This is where confusion explodes ðŸ˜„

### âŒ WRONG interpretation

> â€œClaude remembers 1M tokens foreverâ€

### âœ… CORRECT interpretation

> â€œClaude can see **up to 1M tokens in a single request**â€

Example:

* Entire codebase = 900K tokens
* Prompt = 20K
* Output = 50K

Total:

```
900K + 20K + 50K = 970K âœ”ï¸
```

But next request?

* If you donâ€™t resend that content (or cache it), **itâ€™s gone**

---

# 6ï¸âƒ£ Context window â‰  Memory

This is **very important**:

| Concept        | What it means                      |
| -------------- | ---------------------------------- |
| Context window | Short-term working memory          |
| Chat history   | Just text you keep resending       |
| Model memory   | âŒ Claude has none across API calls |
| RAG / DB       | External long-term memory          |
| Prompt caching | Cost optimization, not memory      |

Claude **does not remember previous calls** unless you send them again.

---

# 7ï¸âƒ£ What happens when you exceed the context window?

Depends on the platform:

### Claude API

* âŒ Request fails with an error
* Nothing is silently dropped

### Chat UI (Claude.ai)

* Old messages may be summarized or truncated automatically
* You usually donâ€™t see it happening

---

# 8ï¸âƒ£ Why large context windows matter (practical reasons)

Large context helps with:

âœ” Whole codebase analysis
âœ” Long research papers
âœ” Multi-document reasoning
âœ” Agents running long workflows
âœ” Fewer hallucinations (because model can â€œseeâ€ more facts)

But:

âš ï¸ Larger context = more cost
âš ï¸ Slower inference
âš ï¸ Not a substitute for databases or search

---

# 9ï¸âƒ£ Cost is based on **tokens inside the window**

You pay for:

* All input tokens
* All output tokens

Example (Sonnet 4.5 standard pricing):

```
180K input tokens
+ 10K output tokens
= 190K tokens billed
```

Even if the model â€œjust readsâ€ something â€” you still pay.

---

# ðŸ”Ÿ Mental model that usually makes it click

Imagine this:

> Claude is **stateless**
> Each request is like handing it a **stack of papers**
> The stack cannot exceed **N tokens**
> Claude reads the entire stack **every time**

If the stack is too big â†’ âŒ rejected

---

# 11ï¸âƒ£ Common misconceptions (quick fixes)

âŒ â€œBigger context = smarter modelâ€
âœ… Bigger context = **more text visible**

âŒ â€œClaude remembers past chatsâ€
âœ… You keep resending them

âŒ â€œ1M tokens means unlimitedâ€
âœ… Hard cap per request

âŒ â€œContext window = output sizeâ€
âœ… Output is **part of** the window

---

# 12ï¸âƒ£ One-line definition (exam-ready)

> **Context window is the maximum number of tokens (input + output) a model can process in a single request.**

---
