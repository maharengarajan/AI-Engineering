### **Text Generation Techniques** 
The methods that control *how* LLMs like GPT generate their next words/tokens.
These techniques directly affect:

‚úî Creativity
‚úî Accuracy
‚úî Randomness
‚úî Repetition
‚úî Quality of outputs

---

# üåü Why Do We Need Text Generation Techniques?

LLMs predict the **next token** from a probability distribution.

Example:
‚ÄúRaining is‚Ä¶‚Äù ‚Üí

* ‚Äúfun‚Äù ‚Üí 0.30
* ‚Äúbad‚Äù ‚Üí 0.25
* ‚Äúcommon‚Äù ‚Üí 0.20
* ‚Äúgood‚Äù ‚Üí 0.10
* others‚Ä¶

If we **always pick the highest probability**, the model becomes:

* Too deterministic
* Repetitive
* Boring

If we **sample randomly**, the model becomes:

* Creative
* But sometimes nonsense

Text generation techniques find the right balance.

---

# üß© The Main Text Generation Techniques

We will go from simple to advanced.

---

# 1Ô∏è‚É£ **Greedy Search (Deterministic)**

Pick the token with the **highest probability every time**.

Example:
Model predicts:
‚ÄúToday is‚Äù ‚Üí

* ‚Äúsunny‚Äù (0.45)
* ‚Äúrainy‚Äù (0.15)
* ‚Äúwindy‚Äù (0.10)

Greedy output:
‚Üí sunny

### ‚úî Pros:

* Fast
* Deterministic
* Predictable

### ‚úò Cons:

* Often gets stuck
* Produces repetitive or low-quality text
* No creativity

Used for:
üîπ Factual QA
üîπ Simple deterministic outputs

---

# 2Ô∏è‚É£ **Beam Search (Deterministic but better)**

Explores **multiple possible sequences** instead of one.

Example:
Beam width = 3 ‚Üí model keeps top 3 best sentence paths.

This helps avoid:

* Bad local choices
* Early mistakes

### ‚úî Pros:

* Better grammar
* Good for translation/summarization

### ‚úò Cons:

* Still deterministic
* Becomes repetitive
* Suppresses creativity
* Not used in modern LLM chat models

---

# 3Ô∏è‚É£ **Temperature Sampling (Controls randomness)**

Temperature controls ‚Äúcreativity.‚Äù

Formula:

```
higher temperature ‚Üí more random  
lower temperature ‚Üí more focused  
```

### üî• Temperature 0.0

* No randomness
* Pure greedy
* Deterministic

### üî• Temperature 0.7 (default for GPT)

* Balanced creativity

### üî• Temperature 1.5

* Very creative
* Can be chaotic

**Example:**

Prompt: ‚ÄúWrite a story about a dragon.‚Äù

* Temp 0.2 ‚Üí factual, boring
* Temp 0.7 ‚Üí creative, nice story
* Temp 1.5 ‚Üí wild, unpredictable adventure

---

# 4Ô∏è‚É£ **Top-k Sampling (Keep only top k tokens)**

Instead of sampling from thousands of possible tokens, choose only the **top k highest-probability tokens**.

Example:
k = 50 ‚Üí choose from best 50 tokens
k = 10 ‚Üí choose from best 10

### ‚úî Pros:

* Reduces noise
* More stable

### ‚úò Cons:

* Still can be repetitive
* Fixed K is not optimal in all contexts

---

# 5Ô∏è‚É£ **Top-p Sampling (Nucleus Sampling) ‚Äì MOST POPULAR**

Also known as **nucleus sampling**.

Instead of picking top-k tokens, it selects tokens from the **smallest set whose combined probability ‚â• p**.

Example:
p = 0.9 ‚Üí use tokens that together cover 90% probability mass.

This means:

* When the model is confident ‚Üí small set of tokens
* When uncertain ‚Üí larger set of tokens

### ‚úî Pros:

* More natural
* Less repetitive
* Balances creativity and accuracy
* Used in ChatGPT, Claude, LLaMA

### ‚úò Cons:

* Slight computational overhead

---

# 6Ô∏è‚É£ **Combined Sampling (Top-k + Top-p)**

Most modern systems combine both.

Why?

* Fine-grained control
* Reduce weird outputs
* Ensure quality even at high creativity

---

# 7Ô∏è‚É£ **Repetition Penalty**

Prevents the model from repeating words, phrases, or entire sentences.

Example (bad behavior without penalty):

```
The cat is a cat and the cat is a cat and the cat‚Ä¶
```

Repetition penalty > 1.0 punishes repeated tokens in the logits.

Used in:

* GPT
* LLaMA
* HuggingFace

---

# 8Ô∏è‚É£ **Frequency & Presence Penalties (OpenAI)**

### üîÑ **Presence penalty**

Discourages tokens already used ‚Üí increases topic diversity.

### üìâ **Frequency penalty**

Discourages frequently repeated tokens ‚Üí reduces repetition.

Useful for:

* Creative writing
* Brainstorming
* Avoiding loops

---

# 9Ô∏è‚É£ **Mixture of Sampling Strategies (used in modern LLMs)**

ChatGPT and similar models internally use:

* temperature
* top-p
* repetition penalty
* context-based heuristics
* alignment constraints

This gives:

* Helpful
* Non-repetitive
* Human-like responses

---

# üéØ Summary Table

| Technique                  | Control           | Creativity        | Typical Use             |
| -------------------------- | ----------------- | ----------------- | ----------------------- |
| Greedy                     | None              | ‚ùå                 | Deterministic tasks     |
| Beam Search                | Multiple paths    | ‚ùå                 | Translation             |
| Temperature                | Randomness        | üå°Ô∏è Adjustable    | Stories, creative tasks |
| Top-k                      | Limits choices    | ‚öñÔ∏è Balanced       | Many apps               |
| Top-p                      | Dynamic sampling  | ‚úî Natural         | Modern LLMs             |
| Repetition Penalty         | Discourages loops | ‚úî Less repetition | Chat models             |
| Frequency/Presence Penalty | Topic diversity   | ‚úî Higher          | Brainstorming           |

---

# üß≥ Simple Explanation (One Sentence)

> Text generation techniques control **how tokens are picked** from the probability distribution, balancing between accuracy (deterministic) and creativity (randomness).

---
### top-k and top-p
Here are **clear, easy-to-understand examples** of **Top-k** and **Top-p (nucleus)** sampling.

To demonstrate this, we‚Äôll use the same prompt:

> **Prompt:** ‚ÄúThe cat chased the‚Äù

And we‚Äôll assume the model produced the following **token probability distribution** (simplified for example):

| Token | Probability |
| ----- | ----------- |
| mouse | 0.40        |
| dog   | 0.20        |
| ball  | 0.15        |
| bird  | 0.10        |
| car   | 0.08        |
| child | 0.05        |
| sun   | 0.02        |

---

# üü¶ **TOP-K Example**

Let‚Äôs choose **k = 3**.

This means the model will keep only the **top 3 highest-probability tokens**.

### Step 1: Pick top 3 tokens

| Token | Probability |
| ----- | ----------- |
| mouse | 0.40        |
| dog   | 0.20        |
| ball  | 0.15        |

All other tokens are **discarded**.

### Step 2: Sample **randomly** from these 3

So the model could generate:

* ‚Äúmouse‚Äù
* ‚Äúdog‚Äù
* ‚Äúball‚Äù

But **never**:

* bird
* car
* child
* sun

Even if sometimes "bird" might be a reasonable continuation, it's excluded because it's outside top-3.

### Sample outputs:

* ‚ÄúThe cat chased the **mouse**‚Äù
* or ‚ÄúThe cat chased the **dog**‚Äù
* or ‚ÄúThe cat chased the **ball**‚Äù

---

# üü© **TOP-P (Nucleus) Example**

Let‚Äôs choose **p = 0.80** (80% of probability mass).

### Step 1: Sort tokens by probability (already sorted).

### Step 2: Keep the **smallest set** of tokens whose cumulative probability ‚â• 0.80.

Let‚Äôs accumulate:

* mouse (0.40) ‚Üí total 0.40
* dog   (0.20) ‚Üí total 0.60
* ball  (0.15) ‚Üí total 0.75
* bird  (0.10) ‚Üí total **0.85 ‚Üí stop (we passed 0.80)**

So the nucleus (allowed tokens) = **{mouse, dog, ball, bird}**

### Key difference:

üëâ Here, 4 tokens are included
üëâ In top-k(k=3), only 3 were included

### Sample outputs:

* ‚ÄúThe cat chased the **mouse**‚Äù
* ‚ÄúThe cat chased the **bird**‚Äù
* ‚ÄúThe cat chased the **dog**‚Äù
* ‚ÄúThe cat chased the **ball**‚Äù

Bird appears here (because combined probabilities needed it), but would **never** appear in top-k(k=3).

---

# üîç **Side-by-Side Comparison**

| Method            | Rule                                  | Included Tokens        | Behavior                   |
| ----------------- | ------------------------------------- | ---------------------- | -------------------------- |
| **Top-k (k=3)**   | Keep fixed top k                      | mouse, dog, ball       | Fixed set ‚Üí less flexible  |
| **Top-p (p=0.8)** | Keep tokens whose cumulative prob ‚â• p | mouse, dog, ball, bird | Dynamic set ‚Üí more natural |

---

# üß† Simple Intuition

### **Top-k:**

> ‚ÄúPick from the K best choices no matter what.‚Äù

### **Top-p (nucleus):**

> ‚ÄúPick from the smallest set of choices that cover P% of the model‚Äôs confidence.‚Äù

Top-p adapts to the model‚Äôs uncertainty, so it's used in almost all modern LLMs.

---

# üß™ Code Example (HuggingFace Transformers)

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

tokenizer = AutoTokenizer.from_pretrained("gpt2")
model = AutoModelForCausalLM.from_pretrained("gpt2")

input_text = "The future of AI is"
input_ids = tokenizer.encode(input_text, return_tensors="pt")

output = model.generate(
    input_ids,
    max_length=50,
    do_sample=True,
    top_k=50,     # keep top 50 tokens
    top_p=0.9,    # from those, keep tokens covering 90% cumulative prob
    temperature=1.0,
)

print(tokenizer.decode(output[0], skip_special_tokens=True))
```

---
# 1. Where temperature fits in text generation

Inside a language model, text generation works like this:

```
Input text ‚Üí Neural network ‚Üí logits ‚Üí softmax ‚Üí probability distribution ‚Üí sample next token
```

**Temperature directly modifies the logits before softmax.**

---

## 2. What are logits?

* The model does **not** output probabilities directly.
* It outputs **logits** ‚Üí raw, unnormalized scores for each token in the vocabulary.

Example (simplified):

| Token   | Logit |
| ------- | ----- |
| "cat"   | 3.0   |
| "dog"   | 2.0   |
| "car"   | 0.5   |
| "apple" | -1.0  |

Higher logit = model prefers that token more.

---

## 3. Softmax (normal behavior)

Softmax converts logits into probabilities:

```
P(token) = exp(logit) / sum(exp(all logits))
```

This creates a probability distribution over tokens.

---

## 4. Temperature: the core idea

**Temperature scales the logits before softmax**

### Formula

```
scaled_logits = logits / temperature
probabilities = softmax(scaled_logits)
```

This is the *only* thing temperature does internally.

---

## 5. What changing temperature actually does

### Case 1: Temperature = 1.0 (default behavior)

```
scaled_logits = logits / 1.0 = logits
```

* Normal probability distribution
* Balanced randomness

---

### Case 2: Temperature < 1 (e.g., 0.5)

```
scaled_logits = logits / 0.5 = logits √ó 2
```

* Differences between logits become **larger**
* High-probability tokens become **much more dominant**
* Low-probability tokens almost disappear

üîπ **Effect**:

* More deterministic
* Less creative
* Repetitive but safer

---

### Case 3: Temperature > 1 (e.g., 1.5)

```
scaled_logits = logits / 1.5
```

* Differences between logits become **smaller**
* Distribution becomes flatter
* Rare tokens get more chance

üîπ **Effect**:

* More randomness
* More creativity
* Higher risk of nonsense

---

### Case 4: Temperature ‚Üí 0 (almost zero)

```
scaled_logits ‚Üí very large differences
```

* Softmax becomes almost a **hard max**
* Always picks the highest-probability token

This is basically **greedy decoding**.

---

## 6. Concrete numeric example

### Original logits

| Token | Logit |
| ----- | ----- |
| A     | 4     |
| B     | 2     |
| C     | 1     |

### Temperature = 1.0

Softmax ‚âà

* A: 0.84
* B: 0.11
* C: 0.05

---

### Temperature = 0.5

Logits √ó 2 ‚Üí `[8, 4, 2]`

Softmax ‚âà

* A: 0.96
* B: 0.03
* C: 0.01

‚û° Very confident, low diversity

---

### Temperature = 2.0

Logits √∑ 2 ‚Üí `[2, 1, 0.5]`

Softmax ‚âà

* A: 0.57
* B: 0.21
* C: 0.22

‚û° Much more diverse

---

## 7. Intuition (simple explanation)

Think of temperature like **confidence vs curiosity**:

* üî• **Low temperature** ‚Üí ‚ÄúI‚Äôm confident, I‚Äôll pick the safest word‚Äù
* üå° **High temperature** ‚Üí ‚ÄúI‚Äôll explore less obvious words‚Äù

---

## 8. What temperature is *based on*

Temperature is based on **statistical mechanics** and **Boltzmann distribution**:

```
P(state) ‚àù exp(-Energy / Temperature)
```

In LLMs:

* Logits ‚âà negative energy
* Softmax with temperature = Boltzmann sampling

So temperature controls **entropy** of the output distribution.

---

## 9. Important clarifications

‚ùå Temperature does **not**:

* Change model weights
* Affect training
* Add new knowledge

‚úÖ It only:

* Changes **sampling behavior**
* Controls randomness at inference time

---

## 10. Temperature vs Top-k / Top-p (quick contrast)

| Parameter   | What it does                                            |
| ----------- | ------------------------------------------------------- |
| Temperature | Scales probabilities                                    |
| Top-k       | Restricts to k most likely tokens                       |
| Top-p       | Restricts to smallest set with cumulative probability p |

Usually used **together**.

---

## 11. Practical guidelines

| Task                 | Temperature |
| -------------------- | ----------- |
| Code generation      | 0.0 ‚Äì 0.3   |
| QA / factual answers | 0.1 ‚Äì 0.4   |
| Chatbots             | 0.6 ‚Äì 0.9   |
| Creative writing     | 0.9 ‚Äì 1.3   |

---

### One-line summary

> **Temperature works by scaling logits before softmax, controlling how peaked or flat the probability distribution is ‚Äî lower = safer, higher = more creative.**