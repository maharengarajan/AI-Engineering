# üîß What is DevOps?

DevOps is not a specific technology or tool. It‚Äôs a culture and methodology for building, testing, and releasing software faster and more reliably.

It integrates:

1. Development (planning, coding, building)
2. Operations (release, deploy, operate, monitor)
3. Quality assurance and security (often expanded to DevSecOps)

# üéØ Why DevOps?
1. Faster time to market
2. Improved collaboration
3. Increased deployment frequency
4. Better quality and stability
5. Reduced failure rate

# üõ†Ô∏è Core DevOps Practices
| Practice                         | Description                                                           |
| -------------------------------- | --------------------------------------------------------------------- |
| **CI/CD**                        | Continuous Integration & Continuous Deployment                        |
| **Infrastructure as Code (IaC)** | Automate infrastructure provisioning (e.g., using Terraform, Ansible) |
| **Monitoring & Logging**         | Real-time insights into system health (e.g., Prometheus, ELK Stack)   |
| **Automated Testing**            | Run tests automatically to detect bugs early                          |
| **Configuration Management**     | Maintain consistent environments (e.g., using Chef, Puppet)           |
| **Version Control**              | Manage source code (e.g., Git)                                        |

---
# üíª What is a Virtual Machine (VM)?
A Virtual Machine (VM) is a software-based emulation of a physical computer.
It runs on a physical machine (called a host) using virtualization software (like VMware, VirtualBox, Hyper-V, or KVM).

üîπ Key Points:
A VM has its own CPU, memory, disk, network, etc., virtually.
You can run multiple VMs on a single physical machine.
Each VM can run a different operating system (Linux, Windows, etc.).
VMs are isolated from each other and the host.

‚úÖ Use Cases:
Testing software on different OS versions.
Running multiple isolated apps on the same hardware.
Hosting applications in cloud environments (e.g., AWS EC2, Azure VMs).

--------
# Linux Operating System
OS is a bridge between software and hardware

### üêß **What is Linux OS?**

**Linux** is a **free and open-source operating system** based on **Unix**. It acts as an interface between the hardware and the applications, just like Windows or macOS.

> Technically, **Linux** is the **kernel**, and full Linux OS distributions (like Ubuntu, CentOS, or Debian) bundle that kernel with software tools and libraries.


### üîß **Why Linux is used on Most Servers?**

Most of the world‚Äôs web servers, cloud infrastructure, supercomputers, and mobile devices (Android) run on Linux. Here's why:


## ‚úÖ **Reasons Why Linux is Preferred for Servers**

### 1. **Open Source & Free**

* No expensive licensing fees.
* You can inspect, modify, and optimize it as per your needs.

### 2. **Stability & Reliability**

* Linux servers can run for **years without crashing or needing a reboot**.
* That‚Äôs critical for web servers and production systems.

### 3. **Security**

* Linux has strong security features like **permissions**, **firewalls**, and **SELinux**.
* Less vulnerable to viruses compared to Windows.

### 4. **Performance**

* Lightweight and fast.
* Uses minimal system resources, ideal for high-performance environments.

### 5. **Customizability**

* You can **strip down** the OS to just what‚Äôs needed (very useful for servers).
* Tools like **Docker**, **Kubernetes**, etc., run smoothly on Linux.

### 6. **Strong Command-Line Interface (CLI)**

* Powerful tools and scripting capabilities (bash, ssh, cron, etc.).
* Makes automation and remote management efficient.

### 7. **Ecosystem & Community Support**

* Massive open-source community.
* Wide range of tools for servers (e.g., Apache, NGINX, MySQL, etc.) are Linux-friendly.

### 8. **Cloud & DevOps Compatibility**

* All major cloud providers (AWS, Azure, GCP) use Linux as default.
* Most DevOps tools and CI/CD pipelines are optimized for Linux.


## üìä Linux vs Other OS for Servers

| Feature      | **Linux**           | **Windows Server**          | **macOS** (rare on servers) |
| ------------ | ------------------- | --------------------------- | --------------------------- |
| Cost         | Free                | Paid license                | Paid (not server-optimized) |
| Customizable | Highly customizable | Limited customization       | Limited                     |
| Stability    | Extremely stable    | Stable, but less than Linux | Not designed for servers    |
| CLI Support  | Excellent           | Decent (PowerShell)         | Not strong                  |
| Security     | Very secure         | Secure but targeted often   | Secure but limited use      |


### üöÄ Summary

* **Linux is ideal for servers** because it's **free, stable, secure, lightweight, and customizable**.
* It powers **over 90% of cloud servers and supercomputers** worldwide.
* Tools like **Apache, Docker, Kubernetes, and Nginx** work best in Linux environments.

---
# Shell Scripting

Shell scripting is the process of writing a script (a series of commands) for a Unix/Linux shell to automate tasks.

Shell commands is way to communicate with Linux OS. In production servers there will not be GUI. In windows if we want to create file we can create using GUI, But in linux it is not possible . The only way is using shell commands we can create a file

üß† What is a Shell?
A shell is a program that takes commands from the user and gives them to the OS to execute.

üîπ Common shells:
Bash (Bourne Again Shell) ‚Äì most popular
Sh (Bourne Shell)
Zsh, Ksh, Fish

# Shell Commands
* touch - create a file
* vim - create and open a file
* cat - print the content of the file
* ls - list the files and folders
* ls -a - list the files and folders (including hidden files)
* chmod - grant permissions
* man - manual command (ex: man ls)
* vim -> esc -> press i (insert) -> esc -> :wq - write in the file and save it
* pwd - present working directory
* mkdir - create folder
* cd - change directory
* cd . . - one step back
* echo - print statement


| **Command**        | **Description**                                                                                          |                                                                          |               |
| ------------------ | -------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------ | ------------- |
| `df -h`            | Displays **disk usage** of mounted filesystems in human-readable format (GB, MB).                        |                                                                          |               |
| `free -h`          | Shows **memory (RAM)** usage, including total, used, free, cache, and swap.                              |                                                                          |               |
| `nproc`            | Displays the **number of CPU cores** available.                                                          |                                                                          |               |
| `top`              | Displays a **real-time view** of running processes, CPU, memory usage (like Task Manager).               |                                                                          |               |
| `ps -ef`           | Shows a **snapshot of all running processes** with full format (`e` = every process, `f` = full format). |                                                                          |               |
| `grep "keyword"`   | Searches for the "keyword" in a file or input. Often used with `ps`, `cat`, etc.                         |                                                                          |               |
| \`                 | \` (pipe)                                                                                                | Passes **output of one command** as input to the next. Example: \`ps -ef | grep python\` |
| `awk '{print $1}'` | Powerful **text processing tool**. Prints the first field/column. Used with pipes.                       |                                                                          |               |

| **Command**       | **Description**                                                                                    |
| ----------------- | -------------------------------------------------------------------------------------------------- |
| `set -x`          | Turns on **debug mode**; shows each command as it is executed. Useful for troubleshooting scripts. |
| `set -e`          | Causes the script to **exit immediately on any error**. Good for catching failures early.          |
| `set -o pipefail` | Makes the script fail if **any part of a pipeline fails**, not just the last one.                  |


| **Command** | **Description**                                                                                  |
| ----------- | ------------------------------------------------------------------------------------------------ |
| `curl URL`  | Transfers data from/to a server (supporting HTTP, FTP, etc.). Can be used to **make API calls**. |
| `wget URL`  | Downloads files from the web via command line. **Simple and useful for large file downloads**.   |


| **Command**                   | **Description**                                                                                    |
| ----------------------------- | -------------------------------------------------------------------------------------------------- |
| `sudo`                        | Executes a command with **superuser privileges**. Needed for admin tasks like installing software. |
| `find /path -name "file.txt"` | Finds files in the directory tree. You can search by name, size, modified time, etc.               |
| `kill PID`                    | Sends a **signal to terminate** a process by its PID (Process ID).                                 |
| `kill -9 PID`                 | Forcefully kills a process (SIGKILL). Use when normal `kill` doesn't work.                         |

| Task                 | Example                                     |
| -------------------- | ------------------------------------------- |
| üîÅ Automation        | Backups, log rotation, job scheduling       |
| üöÄ Deployment        | Starting/stopping services, app deployments |
| üì¶ Installation      | Installing packages or updates              |
| üìÑ File Handling     | Moving, renaming, or compressing files      |
| üíª System Monitoring | CPU, memory, disk usage checks              |
------------------
## Networking 
## üåê What is an IP Address?
An IP address (Internet Protocol address) is a unique identifier assigned to each device connected to a network. It acts like the "home address" of a device on the internet or local network.

It allows devices to:
Identify each other
Send and receive data

## üß† Think of it like...
Just like your house has a postal address to receive letters, your computer has an IP address to send/receive data.

üìú Types of IP Addresses
1. IPv4 (Internet Protocol version 4)
Format: xxx.xxx.xxx.xxx (four numbers, 0‚Äì255)

Example: 192.168.1.10

Most common, but limited to ~4.3 billion addresses

2. IPv6 (Internet Protocol version 6)
Format: Eight groups of four hexadecimal digits

Example: 2001:0db8:85a3:0000:0000:8a2e:0370:7334

Created to solve IPv4 exhaustion; supports trillions of addresses

## üìÇ Categories of IP Addresses
üî∏ A. Public IP
Assigned by your Internet Service Provider (ISP)
Accessible from the internet
Example: 45.120.98.21

## üî∏ B. Private IP
Used within a local network (LAN)
Not accessible from the internet directly
Example: 192.168.0.5, 10.0.0.1
Devices like routers, laptops, and smartphones in your home use private IPs.

---
## üåê What is a Subnet?
A subnet (short for subnetwork) is a smaller, logical division of a larger network. Subnetting is used to divide a large IP network into multiple smaller networks, which improves organization, performance, and security.

## üß† Real-Life Analogy:
Imagine an office building with multiple floors. The building has one main address (like an IP address), but each floor has its own room numbers (subnets) to identify locations within the building.

## üì¶ Why Use Subnetting?
| Benefit                        | Description                                                |
| ------------------------------ | ---------------------------------------------------------- |
| **Better organization**        | Separates departments (HR, IT, Sales) in different subnets |
| **Improved security**          | Restrict traffic between subnets                           |
| **Efficient IP usage**         | Avoid wasting IP addresses                                 |
| **Reduced network congestion** | Limits broadcast domains                                   |

## EX: 192.168.1.0/24
---
### ‚òÅÔ∏è What is a **VPC (Virtual Private Cloud)?**

A **VPC (Virtual Private Cloud)** is a **logically isolated virtual network** in a **cloud environment** (like AWS, Azure, or GCP). It allows you to **launch resources** (like EC2, databases, containers) in a **customized, private network**‚Äîjust like your own data center, but in the cloud.


### üß† Simple Definition:

> A **VPC** is like your **own private data center inside the cloud**. You control its **IP address range**, **subnets**, **routing**, **firewalls**, and **internet access**.


## ‚úÖ **Why Use a VPC?**

| Feature                        | Description                                                          |
| ------------------------------ | -------------------------------------------------------------------- |
| üõ°Ô∏è **Isolation**              | Your VPC is isolated from other users in the cloud                   |
| üéõÔ∏è **Custom IP Ranges**       | Define your own IP space (like `10.0.0.0/16`)                        |
| üîí **Security Controls**       | Use security groups and NACLs (network ACLs) to control traffic      |
| üåç **Internet Access Control** | Choose which resources are public or private                         |
| üåê **Private Connectivity**    | Connect securely to on-premises networks (via VPN or Direct Connect) |


## üß± **Core Components of a VPC (AWS example)**

| Component                  | Purpose                                                       |
| -------------------------- | ------------------------------------------------------------- |
| **CIDR Block**             | IP range of the VPC (e.g., `10.0.0.0/16`)                     |
| **Subnet**                 | Smaller network within the VPC (e.g., public/private subnets) |
| **Route Table**            | Controls routing of traffic within and outside VPC            |
| **Internet Gateway (IGW)** | Enables internet access for public subnets                    |
| **NAT Gateway**            | Allows private subnets to access the internet (outbound only) |
| **Security Group**         | Acts like a firewall for EC2 instances                        |
| **NACL (Network ACL)**     | Optional stateless firewall at the subnet level               |


## üñ•Ô∏è VPC Subnet Types

| Subnet Type         | Description                                          |
| ------------------- | ---------------------------------------------------- |
| **Public Subnet**   | Has access to the internet (via IGW)                 |
| **Private Subnet**  | No direct internet access (can use NAT for outbound) |
| **Isolated Subnet** | No internet access at all (high security)            |


## üåê Example Use Case

Imagine deploying a web application:

* **Web server** in **public subnet** (accessible via internet)
* **Database** in **private subnet** (no direct internet access)
* **NAT Gateway** lets private subnet send updates, etc.

---
## üåê What is a Port?
In computer networking, a port is a logical endpoint for communication. It helps a computer know which service or application should handle a piece of incoming or outgoing data.

Think of it like a door into your system ‚Äî each door (port) leads to a specific room (application/service).

## üß† Real-Life Analogy
Imagine a building (your computer):

The IP address is the building's address
The port number is the apartment or office number in the building
Together, IP:Port lets someone find exactly where to deliver the message

## üì¶ Format - 192.168.1.10:80

---
## Git and GitHub

### üîÅ What is Version Control?

**Version control** is a system that helps developers:

* **Track changes** to code or files over time.
* **Collaborate** with others without overwriting each other‚Äôs work.
* **Revert** back to earlier versions if something goes wrong.

Think of it as a "time machine" for your project.

There are two main types:

* **Centralized Version Control** (e.g., SVN) ‚Äì everything is stored on a central server.
* **Distributed Version Control** (e.g., Git) ‚Äì each user has a full copy of the project history.

### üõ†Ô∏è What is Git?

**Git** is a **distributed version control system** created by Linus Torvalds (creator of Linux) in 2005.

Key features:
* Fast performance.
* Every developer has a local copy of the entire code history.
* Supports branching and merging (ideal for feature development and collaboration).
* Common commands:

  * `git init` ‚Äì initialize a repo
  * `git clone` ‚Äì copy a repo from remote
  * `git add` ‚Äì stage changes
  * `git commit` ‚Äì save changes
  * `git push` ‚Äì upload changes to remote repo
  * `git pull` ‚Äì fetch and merge from remote
  * `git log` ‚Äì fetch the logs


### üåê What is GitHub?

**GitHub** is a **web-based platform** for hosting Git repositories.

Features:

* Stores your Git repositories in the cloud.
* Allows collaboration through **pull requests**, **issues**, **projects**, etc.
* Provides **CI/CD**, **code review**, and **project management tools**.
* Alternatives: GitLab, Bitbucket, Azure Repos.

> Git is the tool (installed on your computer),
> GitHub is the service (hosted online) where you can **store and share your Git repositories**.

---
## üîÄ What is a Git Branch?

A **branch** in Git is a separate line of development. The default branch is usually `main` or `master`.

Branches allow you to:

* Develop features without affecting the main codebase.
* Fix bugs independently.
* Test and experiment safely.

## üå≥ Common Branching Strategies

### 1. **Git Flow**

A very structured strategy, suitable for large teams and projects.

**Main branches:**

* `main`: Production-ready code only.
* `develop`: Integration branch for features before going to production.

**Supporting branches:**

* `feature/*`: For new features (e.g., `feature/login`)
* `bugfix/*`: For bug fixes (e.g., `bugfix/login-issue`)
* `release/*`: Prepares a release (e.g., `release/v1.0`)
* `hotfix/*`: Quick fixes to production (e.g., `hotfix/urgent-fix`)

üëâ Best for: Large-scale applications with scheduled releases.


### 2. **GitHub Flow**

Simpler, widely used with CI/CD and fast delivery pipelines.

**Steps:**

1. Create a branch from `main` (e.g., `feature/search-bar`)
2. Make changes, commit, and push.
3. Open a Pull Request (PR) to `main`.
4. Review, test, and merge after approval.

üëâ Best for: Agile teams, continuous delivery, SaaS projects.


### 3. **Trunk-Based Development**

All developers work directly on `main` (or `trunk`) with **short-lived branches**.

* Feature branches exist for **hours or a day**.
* Merge frequently to avoid drift.
* Use **feature flags** to toggle incomplete code.

üëâ Best for: High-speed development, CI/CD, startups.


## ‚úÖ Best Practices for Branching

1. **Always create branches from the correct base**

   * Features ‚Üí from `develop` (Git Flow) or `main` (GitHub Flow)
   * Hotfixes ‚Üí from `main`

2. **Use consistent naming conventions**

   ```
   feature/login-ui
   bugfix/signup-validation
   hotfix/crash-on-start
   ```

3. **Keep branches short-lived**

   * Merge early and often to avoid merge conflicts.
   * Use `git rebase` for a clean history (optional).

4. **Protect critical branches**

   * Use branch protection rules (e.g., require PR review for `main`)
   * Prevent force pushes to `main` or `develop`

5. **Test before merging**

   * Integrate with CI to automatically test branches on PR.

6. **Use Pull Requests (PRs)**

   * For code review, collaboration, and discussion before merging.

7. **Clean up merged branches**

   * Delete feature branches after merge to keep repo tidy.


## üß™ Quick Visual (GitHub Flow Example)

```
main
  \
   \--- feature/add-payment
          |
          (code, commit, push)
          |
         PR ‚Üí Code Review ‚Üí Merge to main
```

---
## Git Commands
git init - .git will create, 
git add .
git status
git diff
git commit -m "any message" (-m means message)
git log
git push
git remote -v
git remote add ""
git clone
clone vs fork
fork creates a copy of the repository
git checkout -b branch_name
git merge, git rebase, git cherry-pick

---
## AWS Services for DevOps
1. EC2
2. VPC
3. EBS
4. S3
5. IAM
6. Cloud Watch
7. Lambda
8. Cloud Build Services
9. AWS configuration
10. Billing & Costing
11. AWS KMS
12. Cloud Trail
13. EKS
14. Fargate
15. ELK stack

---
### CI/CD Pipeline:

**CI/CD** stands for **Continuous Integration** and **Continuous Delivery/Deployment**. It is a key part of modern DevOps practices used to automate and streamline software development, testing, and deployment.

### üîß What is a CI/CD Pipeline?

A **CI/CD pipeline** is a set of automated steps that help developers:

* Build their code
* Test it
* Merge it with the main project
* Deploy it to production


## üîÅ CI - Continuous Integration

**Goal:** Automatically integrate and test changes to the codebase.

### What Happens in CI:

1. **Developer pushes code** to version control (e.g., GitHub).
2. **Build is triggered** automatically (using tools like Jenkins, GitHub Actions, GitLab CI).
3. **Automated tests** are run (unit tests, integration tests).
4. **Code quality checks** are performed (linters, static analysis).

> ‚úÖ If all tests pass, the code is ready to move forward.


## üöÄ CD - Continuous Delivery vs Continuous Deployment

| Term                      | Description                                                                                                      |
| ------------------------- | ---------------------------------------------------------------------------------------------------------------- |
| **Continuous Delivery**   | Code is automatically prepared for deployment (e.g., pushed to staging), but a human approves actual deployment. |
| **Continuous Deployment** | Code changes are automatically deployed to production with no manual intervention.                               |

### üèóÔ∏è CI/CD Pipeline Stages (Typical Flow)

1. **Source Stage**

   * Triggered by a code change (push/pull request)
   * Pulls latest code from Git repo

2. **Build Stage**

   * Compiles code
   * Installs dependencies

3. **Test Stage**

   * Runs unit, integration, and other tests
   * Checks security vulnerabilities, style, etc.

4. **Package Stage**

   * Creates build artifacts (e.g., `.jar`, `.zip`, Docker image)

5. **Deploy Stage**

   * Deploys the build to staging, QA, or production environments


### üîß Tools Commonly Used

| Purpose             | Tools                                           |
| ------------------- | ----------------------------------------------- |
| CI/CD Orchestration | Jenkins, GitHub Actions, GitLab CI/CD, CircleCI |
| Build & Package     | Maven, Gradle, npm, Docker                      |
| Testing             | JUnit, PyTest, Selenium                         |
| Deployment          | Kubernetes, Ansible, Terraform, AWS CodeDeploy  |


### üîí Benefits of CI/CD

* **Faster releases**
* **Lower risk of bugs**
* **Improved developer productivity**
* **Automated testing & deployment**
* **Quick rollback if something fails**

### üß† Example Use Case

You work on a React app:

* You push new code ‚Üí CI server runs tests
* If tests pass ‚Üí Code is built and Dockerized
* Code is auto-deployed to staging
* QA tests it ‚Üí You click ‚ÄúDeploy‚Äù to move to production (CD)

## CI pipeline best practices
| Check Type           | Tool Examples                 | Purpose                            |
| -------------------- | ----------------------------- | ---------------------------------- |
| Build                | Maven, npm, Gradle            | Ensure code compiles               |
| Unit Tests           | JUnit, pytest, Jest           | Test individual components         |
| Integration Tests    | TestNG, Postman, Pytest       | Validate module interactions       |
| Static Code Analysis | SonarQube, ESLint, flake8     | Detect bugs/code smells early      |
| Security (SAST)      | Semgrep, Checkmarx, GitLeaks  | Static security vulnerability scan |
| Dependency Scan      | Snyk, npm audit, pip-audit    | Check 3rd party libraries          |
| Code Coverage        | JaCoCo, Coverage.py           | Track how much code is tested      |
| Formatting           | Prettier, Black, clang-format | Maintain code consistency          |
| Secret Scanning      | GitLeaks, TruffleHog          | Avoid leaking sensitive info       |
| License Compliance   | FOSSA, LicenseFinder          | Ensure legal compliance            |


## ‚öñÔ∏è SonarQube vs Snyk ‚Äî Comparison Table
| Feature          | **SonarQube**                         | **Snyk**                                  |
| ---------------- | ------------------------------------- | ----------------------------------------- |
| Purpose          | Code quality & static analysis        | Security for dependencies & containers    |
| Scans            | Source code                           | Dependencies, containers, IaC             |
| Focus            | Bugs, smells, code coverage, security | Known vulnerabilities (CVEs), licenses    |
| Languages        | 25+ (Java, Python, JS, etc.)          | Based on package manager (npm, pip, etc.) |
| Integration      | Jenkins, GitHub, GitLab, Bitbucket    | GitHub, GitLab, Docker Hub, Jenkins       |
| License Analysis | No                                    | Yes                                       |
| Secret Detection | No                                    | Partial (in containers)                   |

## üß© What Is a Multi-Stage Pipeline?

A **multi-stage pipeline** breaks the CI/CD process into **distinct stages**, each with its own purpose and controls.
This allows for **better isolation, security, testing, and approval workflows**.

### ‚úÖ Example Stages

1. **Build**
2. **Test**
3. **Static Analysis**
4. **Security Scan**
5. **Deploy to Dev**
6. **Approval**
7. **Deploy to Staging**
8. **Deploy to Production**

Each stage:

* Can have its own environment variables
* Can run conditionally (e.g., only on `main` branch)
* Can require **manual approvals** (for prod)
* Is **fail-fast** ‚Äî stops the pipeline if errors occur


## üß† Real-World Example (Multi-Stage)

```yaml
stages:
  - name: Build
  - name: Test
  - name: Deploy

jobs:
  - stage: Build
    steps:
      - Compile code
      - Package artifacts

  - stage: Test
    steps:
      - Run unit tests
      - Run integration tests

  - stage: Deploy
    steps:
      - Deploy to dev environment
```


## ü§ñ What Is a Multi-Agent Pipeline?

A **multi-agent pipeline** uses **multiple build agents (runners)** to perform pipeline jobs **in parallel or on different environments**.

### üõ†Ô∏è Agent = A machine (VM/container) that runs CI/CD jobs.

### Why Use Multiple Agents?

* Speed up the pipeline by **parallel execution**
* Run steps on **different platforms** (e.g., Linux vs Windows)
* Use **specialized agents** (e.g., one with Docker, another with Android SDK)
* Isolate resource-intensive tasks


### üì¶ Example: Multi-Agent CI/CD Pipeline

| Stage         | Agent Type         | Purpose                      |
| ------------- | ------------------ | ---------------------------- |
| Build         | Ubuntu Linux agent | Compile backend (Java)       |
| Test          | Windows agent      | Run .NET tests               |
| Frontend Test | Node.js agent      | Run React unit tests         |
| Docker Push   | Docker-in-Docker   | Build and push container     |
| Deploy        | AWS-hosted runner  | Deploy to staging/production |


## üö¶ Combined: Multi-Stage, Multi-Agent CI/CD

You're combining **structured stages** with **distributed execution**, like this:

### üîÅ Flow:

1. **Build (Agent A)**

   * Compile code
   * Generate artifacts

2. **Test (Agent B & C in parallel)**

   * Backend tests (Agent B)
   * Frontend tests (Agent C)

3. **Scan (Agent D)**

   * SonarQube or Snyk scan

4. **Deploy to Dev (Agent E)**

   * Deploy to Dev environment

5. **Manual Approval**

6. **Deploy to Production (Agent F)**

   * Production deploy


## üîí Benefits

| Benefit                   | Description                         |
| ------------------------- | ----------------------------------- |
| ‚úÖ Better separation       | Logical grouping of pipeline phases |
| ‚úÖ Faster execution        | Parallel jobs via multiple agents   |
| ‚úÖ Environment flexibility | Different tools/OS per agent        |
| ‚úÖ Improved security       | Isolate critical deployment agents  |
| ‚úÖ Scalable & maintainable | Easier to debug and update stages   |


## üß™ Supported By

* **Azure DevOps**: Has built-in multi-stage pipelines with agent pools.
* **GitLab CI**: Supports stages and runners.
* **GitHub Actions**: Supports jobs with `runs-on` for different agents.
* **Jenkins**: Can use labels and stages in declarative pipeline.


## ‚úÖ TL;DR

| Term            | Meaning                                                       |
| --------------- | ------------------------------------------------------------- |
| **Multi-Stage** | Organize CI/CD into logical, isolated phases                  |
| **Multi-Agent** | Run jobs on different machines (parallel or OS/tool specific) |
| **Combined**    | Optimized, scalable, and flexible CI/CD pipeline setup        |

## Advantages of GitHub Actions over Jenkins
Hosting: Jenkins is self-hosted, meaning it requires its own server to run, while GitHub Actions is hosted by GitHub and runs directly in your GitHub repository.

User interface: Jenkins has a complex and sophisticated user interface, while GitHub Actions has a more streamlined and user-friendly interface that is better suited for simple to moderate automation tasks.

Cost: 
Jenkins can be expensive to run and maintain, especially for organizations with large and complex automation needs. GitHub Actions, on the other hand, is free for open-source projects and has a tiered pricing model for private repositories, making it more accessible to smaller organizations and individual developers.

## Advantages of Jenkins over GitHub Actions
Integration: Jenkins can integrate with a wide range of tools and services, but GitHub Actions is tightly integrated with the GitHub platform, making it easier to automate tasks related to your GitHub workflow.

In conclusion, 
Jenkins is better suited for complex and large-scale automation tasks, while GitHub Actions is a more cost-effective and user-friendly solution for simple to moderate automation needs.

## üèÉ‚Äç‚ôÇÔ∏è What Is a ‚ÄúRunner‚Äù in GitHub Actions?

A **runner** is a server (machine) that executes your CI/CD jobs from a GitHub Actions workflow.
When a workflow is triggered (e.g., on `git push`), the runner:

1. Picks up the job
2. Executes each step in the workflow
3. Reports logs and results back to GitHub


## ‚ö° GitHub-Hosted Runners

These are **provided and maintained by GitHub**.

### ‚úÖ Features:

* Pre-configured virtual machines

  * OS options: `ubuntu-latest`, `windows-latest`, `macos-latest`
* Pre-installed with common tools (Java, Node.js, Docker, etc.)
* Automatically created and destroyed per job (ephemeral)
* No setup or maintenance required

### ‚ö†Ô∏è Limitations:

* Limited runtime (default 6 hours per job)
* Limited CPU/RAM (depends on plan, e.g., 2-core/7GB for free tier)
* Internet-accessible only (no direct access to private corporate networks unless configured)
* May cost money if you exceed free tier minutes


## üñ•Ô∏è Self-Hosted Runners

These are **machines you manage yourself** that connect to GitHub Actions.

### ‚úÖ Features:

* Runs on your own server, VM, or even local machine
* Can have **custom hardware** (e.g., GPU for ML jobs)
* Can run inside **private networks** (access to internal DBs, services)
* No per-minute billing from GitHub (but you pay for hardware/hosting)
* Full control over installed software and configuration

### ‚ö†Ô∏è Limitations:

* You must maintain, update, and secure the machine
* If it goes down, your workflows fail
* More complex setup (register runner with GitHub repo/org)


## ‚öñÔ∏è Comparison Table

| Feature                 | GitHub-Hosted Runner      | Self-Hosted Runner              |
| ----------------------- | ------------------------- | ------------------------------- |
| Setup & Maintenance     | None (ready to use)       | You manage it                   |
| Cost                    | Free minutes + paid extra | Your hosting cost               |
| Speed                   | Medium (shared infra)     | Can be faster (custom hardware) |
| OS/Env Flexibility      | Fixed images only         | Any OS / hardware you want      |
| Security                | GitHub managed            | You manage security & patches   |
| Access to Private Infra | ‚ùå No (without tunneling)  | ‚úÖ Yes                           |
| Custom Software         | Limited pre-installed     | Fully customizable              |


## üèÜ Which One Is Best?

**It depends on your use case:**

| Choose **GitHub-Hosted** if:                        |
| --------------------------------------------------- |
| You want zero maintenance                           |
| Your workloads fit standard OS images               |
| You don‚Äôt need private network access               |
| You want quick scaling without worrying about infra |

| Choose **Self-Hosted** if:                                         |
| ------------------------------------------------------------------ |
| You need access to private/internal systems                        |
| You need GPUs or special hardware                                  |
| You need very fast builds with custom environments                 |
| You want to avoid GitHub minute costs (if running a lot of builds) |


üí° **Hybrid Approach** is common in large companies:

* Use **GitHub-hosted** for general builds/tests.
* Use **self-hosted** for heavy workloads, ML training, private network deployments.

---
## Containers
In **DevOps**, **containers** are a lightweight, portable way to package and run applications so they behave the same regardless of where they‚Äôre deployed‚Äîwhether on a developer‚Äôs laptop, a test server, or production in the cloud.

Think of them as **"virtual boxes"** for your app, but much smaller and faster than virtual machines (VMs).


## **Key Points about Containers in DevOps**

1. **Definition**
   A **container** bundles:

   * Your application code
   * Dependencies (libraries, runtime, config files)
   * Environment variables
     All in a single **image** so it can run consistently anywhere.

2. **Why Containers in DevOps?**

   * **Consistency** ‚Üí No ‚Äúworks on my machine‚Äù problems.
   * **Portability** ‚Üí Same container runs on laptop, on-prem, or cloud.
   * **Speed** ‚Üí Start in seconds (unlike VMs that take minutes).
   * **Scalability** ‚Üí Easy to replicate for high traffic using orchestration tools (Kubernetes, ECS, etc.).
   * **Isolation** ‚Üí Each container runs in its own environment, avoiding conflicts.

3. **Popular Container Tools**

   * **Docker** ‚Üí The most common container platform.
   * **Podman**, **containerd**, **CRI-O** ‚Üí Alternatives to Docker.
   * **Kubernetes** ‚Üí Orchestrates and manages containers at scale.
   * **OpenShift**, **Amazon ECS**, **Azure AKS** ‚Üí Managed container services.

4. **How Containers Work in DevOps Pipeline**

   * **Build Stage** ‚Üí Code + dependencies are packaged into a container image.
   * **Test Stage** ‚Üí Container image is tested in isolated environments.
   * **Deploy Stage** ‚Üí Same container image is deployed across environments.
   * **Monitor Stage** ‚Üí Health and logs of running containers are tracked.

5. **Containers vs Virtual Machines**

   | **Feature**  | **Container**         | **Virtual Machine** |
   | ------------ | --------------------- | ------------------- |
   | Startup time | Seconds               | Minutes             |
   | Size         | MBs                   | GBs                 |
   | OS           | Shares host OS kernel | Full guest OS       |
   | Performance  | Near-native           | Slight overhead     |
   | Portability  | High                  | Medium              |


üí° **Example**:
If you build a Node.js app, instead of installing Node.js and dependencies separately on each environment, you create a **Docker image** containing:

```Dockerfile
FROM node:20
WORKDIR /app
COPY package*.json .
RUN npm install
COPY . .
CMD ["npm", "start"]
```

Now this container runs exactly the same on any machine with Docker installed.

## what is Docker - is a conteinerization platform
**Docker** is a **platform** that lets you build, package, and run applications inside **containers**.

In simple terms:
Docker takes your application code, libraries, dependencies, and environment settings, bundles them into a **Docker image**, and runs that image as a **container** anywhere‚Äîwithout worrying about setup differences.


## **Why Docker?**

Before Docker:

* Developers often said, *‚ÄúIt works on my machine‚Äù*, but failed in testing or production because of different environments.
* Setting up dependencies was manual and time-consuming.

With Docker:

* The environment is packaged with the application.
* It runs the same on **local**, **test**, and **production**.


## **Core Concepts in Docker**

1. **Docker Image**

   * Blueprint of your application (code + environment).
   * Built from a **Dockerfile**.
   * Example: `myapp:v1`

2. **Docker Container**

   * A running instance of a Docker image.
   * Lightweight, isolated, and portable.

3. **Dockerfile**

   * A text file with instructions to build a Docker image.
   * Example:

     ```Dockerfile
     FROM python:3.10
     WORKDIR /app
     COPY requirements.txt .
     RUN pip install -r requirements.txt
     COPY . .
     CMD ["python", "app.py"]
     ```

4. **Docker Hub**

   * Public registry for sharing Docker images.
   * Similar to GitHub but for containers.

5. **Docker Engine**

   * The runtime that builds and runs containers.


## **How Docker Fits into DevOps**

* **In CI/CD** ‚Üí Build once, run anywhere.
* **In Testing** ‚Üí Spin up identical test environments instantly.
* **In Deployment** ‚Üí Deploy same container to AWS, Azure, GCP, or on-prem without changes.
* **In Scalability** ‚Üí Works seamlessly with orchestration tools like Kubernetes.

## **Docker Workflow Example**

1. Write code ‚Üí Create a `Dockerfile`.
2. Build image ‚Üí `docker build -t myapp:v1 .`
3. Run container ‚Üí `docker run -p 8080:8080 myapp:v1`
4. Push to registry ‚Üí `docker push username/myapp:v1`
5. Deploy in production ‚Üí Pull and run the same image.

‚úÖ **In short:**
Docker is the **tool** that makes containers practical and easy to use, while containers are the **concept** of packaging apps in isolated, portable environments.

## Multi Stage Docker Bild
A **multi-stage Docker build** is a technique where you use **multiple `FROM` statements** in a single `Dockerfile` so that you can separate **build** and **runtime** environments.

The main goal:

* **Build stage** ‚Üí Compile or prepare your app (includes all build tools, compilers, etc.).
* **Final stage** ‚Üí Copy only the needed artifacts into a smaller, cleaner image (without all the build tools).

This results in **smaller, more secure, and faster images**.

## **Why Use Multi-Stage Builds?**

1. **Reduce Image Size** ‚Üí Only the final output goes into production.
2. **Improve Security** ‚Üí Build tools and unnecessary files don‚Äôt exist in production image.
3. **Cleaner & Easier Maintenance** ‚Üí No need to manually remove build dependencies.

## **Example**

### Without Multi-Stage

```Dockerfile
FROM golang:1.20
WORKDIR /app
COPY . .
RUN go build -o myapp .
CMD ["./myapp"]
```

* Problem: The final image contains **Go compiler**, build tools, and source code ‚Äî making it **large**.


### With Multi-Stage

```Dockerfile
# Stage 1: Build
FROM golang:1.20 AS builder
WORKDIR /app
COPY . .
RUN go build -o myapp .

# Stage 2: Runtime
FROM alpine:3.18
WORKDIR /app
COPY --from=builder /app/myapp .
CMD ["./myapp"]
```

**What happens here?**

1. **Stage 1** (`builder`)

   * Uses `golang:1.20` image (has compiler & tools).
   * Builds `myapp` binary.

2. **Stage 2** (final runtime image)

   * Uses small `alpine` image.
   * Copies **only the compiled binary** from Stage 1.
   * No compiler, no extra files ‚Üí drastically smaller image size.


## **Benefits in Numbers**

* Without multi-stage: **\~1 GB** image size.
* With multi-stage: **\~15 MB** image size.

üí° **In DevOps pipelines**, multi-stage builds are common for:

* **Go, Java, Node.js, Python** apps where build and runtime environments differ.
* **Minimizing deployment size** for faster CI/CD delivery.

## Distroless Images
**Distroless** refers to a type of Docker image that contains **only your application and its runtime dependencies** ‚Äî **no full operating system**, package manager, or shell like `/bin/bash`.

It was introduced by **Google** to create **smaller, more secure, and minimal container images**.

## **Why "Distroless"?**

Normally, base images like `ubuntu`, `debian`, or `alpine` come with:

* A full OS filesystem
* Package managers (`apt`, `yum`, `apk`)
* Shells (`bash`, `sh`)
* Tools (`curl`, `ping`, `vi`, etc.)

With **Distroless**:

* All that‚Äôs gone ‚Äî you only keep what's necessary to run the app (e.g., Java runtime, Python interpreter).
* Attack surface is reduced.
* Image size is smaller.


## **Key Characteristics**

1. **No Package Manager** ‚Üí Can‚Äôt install software inside the container at runtime.
2. **No Shell** ‚Üí Can‚Äôt `exec bash` or run shell commands.
3. **Smaller Attack Surface** ‚Üí Fewer possible vulnerabilities.
4. **Language-specific variants** ‚Üí Images for Java, Python, Node.js, Go, etc.


## **Example**

### Using Debian (Traditional)

```dockerfile
FROM debian:11
WORKDIR /app
COPY myapp .
CMD ["./myapp"]
```

* Includes **everything Debian offers**: package manager, shell, extra binaries.
* Larger image size.


### Using Distroless

```dockerfile
FROM gcr.io/distroless/base
WORKDIR /app
COPY myapp .
CMD ["./myapp"]
```

* No shell, no extra tools ‚Äî just enough to run your binary.
* Much smaller & safer.


## **Why Use It in DevOps?**

* **Security** ‚Üí Fewer CVEs (Common Vulnerabilities and Exposures).
* **Compliance** ‚Üí Easier to pass security scans.
* **Performance** ‚Üí Smaller pull/push times in CI/CD.
* **Best Practice** ‚Üí "Build in one container, run in a minimal runtime image" (often paired with **multi-stage builds**).


## **Real Example: Multi-Stage + Distroless**

```dockerfile
# Stage 1: Build
FROM golang:1.21 AS builder
WORKDIR /app
COPY . .
RUN go build -o myapp .

# Stage 2: Distroless runtime
FROM gcr.io/distroless/base
WORKDIR /app
COPY --from=builder /app/myapp .
CMD ["./myapp"]
```

* **Stage 1** ‚Üí Uses full Go compiler.
* **Stage 2** ‚Üí Uses minimal Distroless image for runtime.


**‚ö†Ô∏è Note:**
Since Distroless images don‚Äôt have a shell, debugging inside them is harder. If debugging is needed, use a bigger image temporarily in dev.

## Comparision
Here‚Äôs the **comparison table** for **Alpine vs Distroless vs Ubuntu** so you can quickly decide which base image to use in DevOps work:

| Feature / Criteria        | **Ubuntu**                                                    | **Alpine**                                                                      | **Distroless**                                                                                 |
| ------------------------- | ------------------------------------------------------------- | ------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------- |
| **Image Size**            | Large (\~70‚Äì120 MB)                                           | Very small (\~5 MB)                                                             | Small (\~20‚Äì30 MB, language/runtime dependent)                                                 |
| **Includes OS Tools**     | ‚úÖ Yes (full GNU/Linux utilities)                              | ‚úÖ Yes (minimal BusyBox utilities)                                               | ‚ùå No (only runtime libraries & app)                                                            |
| **Package Manager**       | ‚úÖ `apt`                                                       | ‚úÖ `apk`                                                                         | ‚ùå None                                                                                         |
| **Shell Access**          | ‚úÖ `/bin/bash` or `/bin/sh`                                    | ‚úÖ `/bin/sh`                                                                     | ‚ùå None                                                                                         |
| **Ease of Debugging**     | ‚úÖ Very easy (has shell & tools)                               | ‚ö† Limited (basic tools only)                                                    | ‚ùå Hard (no shell or tools)                                                                     |
| **Security Surface**      | ‚ùå Larger (more packages, more CVEs)                           | ‚ö† Small but still has shell & tools                                             | ‚úÖ Minimal (fewest attack vectors)                                                              |
| **Performance (Startup)** | ‚ùå Slower (larger size)                                        | ‚úÖ Fast                                                                          | ‚úÖ Fast                                                                                         |
| **Use Cases**             | - Development base image <br> - When full OS tools are needed | - Lightweight general-purpose containers <br> - CI/CD jobs, small microservices | - Production runtime only <br> - High-security workloads <br> - Paired with multi-stage builds |
| **Examples**              | `FROM ubuntu:22.04`                                           | `FROM alpine:3.19`                                                              | `FROM gcr.io/distroless/base` or `FROM gcr.io/distroless/java`                                 |


### **Quick Decision Guide**

* **Use Ubuntu** ‚Üí When you need a familiar, full-featured OS environment, lots of debugging tools, or complex dependency installation.
* **Use Alpine** ‚Üí When you want a small image but still need shell, minimal utilities, and some package management.
* **Use Distroless** ‚Üí For **final production runtime** in secure environments ‚Äî especially when paired with **multi-stage builds** to strip everything except the app.

## üîπ **What is a Docker Volume?**

A **volume** is a special storage mechanism in Docker, managed by Docker itself, that allows containers to read/write data **persistently**.

By default, when a container writes data to its filesystem:

* The data is **lost** if the container is removed (`docker rm`).
* That‚Äôs because containers are **ephemeral** (short-lived).

üëâ **Volumes solve this problem** by keeping data outside the container‚Äôs lifecycle.


## üîπ **Why Do We Need Docker Volumes?**

1. **Data Persistence** ‚Üí Store logs, database files, configs that survive container restarts/replacements.
2. **Sharing Data Between Containers** ‚Üí Multiple containers can access the same volume (e.g., app + database).
3. **Decouple Storage from Container** ‚Üí You can delete/upgrade containers without losing data.
4. **Performance** ‚Üí Volumes are optimized by Docker (faster than bind mounts in many cases).
5. **Backup & Portability** ‚Üí Volumes can be backed up, restored, or moved across environments.


## üîπ **Types of Docker Storage**

1. **Volumes** (preferred)

   * Managed by Docker.
   * Stored in `/var/lib/docker/volumes/...` on the host.
   * Example:

     ```bash
     docker volume create mydata
     docker run -v mydata:/app/data myapp
     ```

2. **Bind Mounts**

   * Maps a host directory into a container.
   * Example:

     ```bash
     docker run -v /host/path:/container/path myapp
     ```

3. **Tmpfs Mounts**

   * Stores data **in memory only** (not written to disk).
   * Data disappears when the container stops.
   * Used for sensitive or temporary data.


## üîπ **Example Use Case: Database**

Let‚Äôs say you run **MySQL** in Docker:

```bash
docker run -d \
  --name mysql \
  -e MYSQL_ROOT_PASSWORD=secret \
  -v mysql_data:/var/lib/mysql \
  mysql:8
```

* `-v mysql_data:/var/lib/mysql`
  ‚Üí This stores MySQL data in a **named volume** called `mysql_data`.
* If the container crashes or you upgrade MySQL, the data is still safe in the volume.


## üîπ **Volumes in DevOps Pipelines**

* During **CI/CD**, containers are often replaced or redeployed ‚Üí volumes ensure **persistent data**.
* Useful for:

  * Database storage (Postgres, MySQL, MongoDB)
  * Application logs
  * Configuration files
  * Shared caches between builds


‚úÖ **In short:**
Docker volumes are needed because **containers are temporary, but data should not be**. They provide a safe, consistent way to store and share data beyond a single container‚Äôs lifecycle.

## üöÄ Docker Networking

When you run containers, each one has its own **isolated network stack** (IP, hostname, ports). Docker networking provides the plumbing that connects them.


# üîπ **Types of Docker Networks**

Docker comes with several built-in drivers:

### 1. **Bridge (default)**

* Default network if you don‚Äôt specify one.
* Containers on the same bridge can talk to each other via IP or container name.
* Containers are isolated from the host unless you expose ports (`-p`).

Example:

```bash
docker network create mybridge
docker run -d --name app1 --network mybridge nginx
docker run -d --name app2 --network mybridge alpine ping app1
```

‚û° `app2` can reach `app1` by name (`app1`).


### 2. **Host**

* Removes network isolation between the container and the host.
* Container shares the **host‚Äôs network namespace**.
* No need for `-p` port mapping ‚Äî ports are directly exposed on the host.

Example:

```bash
docker run --network host nginx
```

‚û° The container runs on the host‚Äôs IP directly.

‚ö†Ô∏è Used when performance is critical or apps need direct access to host network.


### 3. **None**

* Completely isolated ‚Äî no network interface except loopback (`lo`).
* Useful for security or when you attach custom networks later.

Example:

```bash
docker run --network none nginx
```

---

### 4. **Overlay**

* Used in **Docker Swarm** or multi-host setups.
* Allows containers on **different hosts** to communicate securely.
* Uses VXLAN tunneling under the hood.

Example (Swarm mode):

```bash
docker network create -d overlay myoverlay
```


### 5. **Macvlan**

* Assigns a **MAC address** to each container so it appears as a physical device on the LAN.
* Containers get IPs from the same network as your host.
* Useful when containers need to look like real devices on the network (e.g., legacy apps).


# üîπ **Common Use Cases in DevOps**

* **Bridge** ‚Üí Default choice for local development and container-to-container communication.
* **Host** ‚Üí Performance-critical apps (e.g., monitoring, logging agents).
* **Overlay** ‚Üí Multi-host networking in Swarm or Kubernetes.
* **Macvlan** ‚Üí When containers need unique IPs on corporate LAN.
* **None** ‚Üí Security-sensitive workloads.


# üîπ **Example: Web App + DB**

```bash
docker network create mynet

docker run -d --name db --network mynet mysql:8
docker run -d --name web --network mynet -p 8080:80 nginx
```

* Both `db` and `web` are in `mynet`.
* `web` can reach `db` via `db:3306`.
* Outside world accesses `web` at `http://localhost:8080`.


# üîπ **Inspecting Networks**

```bash
docker network ls         # List networks
docker network inspect mynet   # Details of network
docker network connect mynet container1   # Attach container
docker network disconnect mynet container1 # Detach container
```


‚úÖ **In short:**
Docker networking lets containers **talk to each other, the host, and external systems**.

* **Bridge** ‚Üí Default local network
* **Host** ‚Üí Direct host network access
* **None** ‚Üí Fully isolated
* **Overlay** ‚Üí Multi-host communication
* **Macvlan** ‚Üí Containers act as real devices

---
### üîπ What is **Kubernetes**?

Kubernetes (often abbreviated as **K8s**) is an **open-source container orchestration platform** developed by Google and now maintained by the **Cloud Native Computing Foundation (CNCF)**.

It helps you **deploy, scale, and manage containerized applications** automatically across a cluster of servers.

üëâ In short: Kubernetes is like an **operating system for your containers**.

### üîπ Why is Kubernetes Important?

1. **Container Management at Scale**

   * Containers (like Docker) are lightweight and portable, but managing hundreds or thousands of them manually is a nightmare.
   * Kubernetes automates this ‚Äî it decides where containers run, restarts failed ones, and balances load.

2. **High Availability & Self-Healing**

   * If a container or node crashes, Kubernetes automatically restarts or replaces it.
   * Ensures apps stay available with minimal downtime.

3. **Scalability**

   * Kubernetes can automatically scale apps **up or down** based on demand (e.g., more users hitting your website).

4. **Portability & Cloud Agnostic**

   * Works across **on-premises**, **AWS**, **Azure**, **GCP**, or even hybrid setups.
   * Avoids vendor lock-in.

5. **Load Balancing & Service Discovery**

   * Routes traffic smartly so no single container gets overloaded.
   * Provides built-in **DNS/service discovery** for microservices.

6. **Efficient Resource Utilization**

   * Places containers intelligently on nodes to make the best use of CPU, memory, and storage.

7. **Declarative Infrastructure (Infrastructure as Code)**

   * You define the **desired state** (e.g., ‚ÄúI want 5 replicas of this app‚Äù), and Kubernetes makes sure it happens.

‚úÖ **Summary in one line**:
Kubernetes is important because it **takes the complexity out of running containerized applications at scale**, ensuring they are always **available, scalable, and portable across environments**.

## üîπ **Kubernetes (K8s) architecture**:

Kubernetes follows a **master‚Äìworker architecture**:

* **Control Plane (Master Node)** ‚Üí The **brain** of Kubernetes (makes decisions).
* **Worker Nodes** ‚Üí The **muscle** of Kubernetes (runs your apps inside containers).

## üîπ Components of Kubernetes Architecture

### 1. **Control Plane (Master Node)** üß†

The control plane manages the **cluster state** ‚Äî it decides *what* should run and *where*.

Key components:

1. **API Server**

   * The **entry point** for all commands (via `kubectl` or API calls).
   * Validates requests and updates the cluster state.

2. **etcd**

   * A **distributed key-value store** that stores the entire cluster state (what pods are running, configs, secrets).
   * Think of it as Kubernetes‚Äô **database**.

3. **Scheduler**

   * Assigns **pods** (your app containers) to worker nodes.
   * Decides placement based on CPU, RAM, affinity, etc.

4. **Controller Manager**

   * Watches the cluster and ensures the **desired state = actual state**.
   * Example: If you say ‚ÄúI need 3 replicas,‚Äù and 1 pod dies, it creates a new one.

### 2. **Worker Node** üí™

This is where your apps actually run. Each worker node runs:

1. **Kubelet**

   * Agent running on each worker node.
   * Talks to the API server and ensures pods are running inside containers.

2. **Container Runtime**

   * Software that runs containers (e.g., Docker, containerd, CRI-O).

3. **Kube Proxy**

   * Handles **networking**.
   * Forwards requests to the right pods, does load balancing inside the cluster.


### 3. **Pods** (Smallest Unit in K8s) üß©

* A **pod = 1 or more containers** that run together, share storage/network.
* You don‚Äôt deploy containers directly ‚Üí you deploy **pods**.


### 4. **Cluster Add-Ons**

* **DNS** ‚Üí For service discovery.
* **Ingress Controller** ‚Üí For external traffic (HTTP/HTTPS routing).
* **Dashboard / Monitoring (Prometheus, Grafana)** ‚Üí Observability.


## üîπ Kubernetes Architecture Diagram (Text Version)

```
                  Control Plane (Master Node)
                  ----------------------------
     +------------+   +---------+   +------------------+
     |  API Server|   |  etcd   |   | Controller Manager|
     +------------+   +---------+   +------------------+
            |                   |              |
            |                   |              |
     +-----------------------------------------------+
     |                  Scheduler                    |
     +-----------------------------------------------+

                  Worker Nodes (Data Plane)
                  --------------------------
     +-----------------------------------------------+
     |  Kubelet |  Kube Proxy | Container Runtime    |
     |     +-------------------------------------+   |
     |     |              Pods                   |   |
     |     |   (Containers running your app)     |   |
     +-----------------------------------------------+
```


## üîπ Flow of How It Works

1. You (developer) tell Kubernetes:

   ```yaml
   apiVersion: v1
   kind: Deployment
   spec:
     replicas: 3
     ...
   ```

   ‚Üí via `kubectl apply -f app.yaml`

2. **API Server** receives the request.

3. **etcd** stores this desired state.

4. **Scheduler** decides which worker node should run the pods.

5. **Kubelet** on that worker node pulls the Docker image and starts the container.

6. **Kube Proxy** exposes it to the network so other services/users can talk to it.


‚úÖ **In short:**

* **Control Plane = Brain (decides what/where to run)**
* **Worker Nodes = Muscles (actually run your containers in pods)**
* **Pods = Smallest unit that runs your app**

## üîπ What is a Kubernetes Service?

In Kubernetes, **pods are ephemeral** (they can die, restart, or move to another node). Each pod also gets a **new IP address** when recreated.

üëâ This creates a problem: **How do other pods or external users reliably access your app if pod IPs keep changing?**

‚úÖ That‚Äôs where **Kubernetes Services** come in.
A **Service** is a stable, permanent networking abstraction that:

* Gives a **fixed IP + DNS name**
* **Load balances** traffic across pods
* Allows communication **inside** and **outside** the cluster

## üîπ Types of Kubernetes Services

### 1. **ClusterIP (default)**

* Exposes the service **inside the cluster only** (internal communication).
* Gets a **stable virtual IP** (but no external access).
* Used when services need to talk **to each other**.

üëâ Example:

* Your backend pod (`backend-svc`) is accessed only by the frontend pod.

```
Frontend Pod  --->  backend-svc (ClusterIP) ---> Backend Pods
```

### 2. **NodePort**

* Exposes the service on a **static port** on each worker node.
* Accessible from **outside the cluster** using `NodeIP:NodePort`.
* Range: `30000-32767`.
* Not used much in production (better to use LoadBalancer or Ingress).

üëâ Example:

```
http://<NodeIP>:30080 --> NodePort Service --> Pods
```


### 3. **LoadBalancer**

* Works with cloud providers (AWS, Azure, GCP).
* Automatically provisions a **cloud load balancer** and routes traffic to pods.
* Easiest way to expose an app to the internet.

üëâ Example:

```
Internet ---> Cloud Load Balancer ---> k8s Service (LoadBalancer) ---> Pods
```


### 4. **ExternalName**

* Maps a Kubernetes service to an **external DNS name** (like `api.stripe.com`).
* Useful for connecting cluster services to external APIs.


## üîπ How a Service Works Internally

* Kubernetes uses **kube-proxy** on worker nodes.
* `kube-proxy` maintains **iptables rules / IPVS rules** to forward traffic.
* When you hit the service, kube-proxy load-balances traffic across healthy pods (via labels & selectors).


## üîπ Example YAML for a Service

```yaml
apiVersion: v1
kind: Service
metadata:
  name: myapp-service
spec:
  selector:
    app: myapp       # which pods this service targets
  ports:
    - protocol: TCP
      port: 80       # service port
      targetPort: 8080  # pod port
  type: ClusterIP    # can be NodePort / LoadBalancer
```

## ‚úÖ Summary

* **ClusterIP** ‚Üí Internal only (default).
* **NodePort** ‚Üí Access via `<NodeIP>:<Port>` (basic external access).
* **LoadBalancer** ‚Üí Best for cloud external access (auto LB provisioned).
* **ExternalName** ‚Üí Maps service to external DNS.

üëâ In practice:

* **Microservices talk to each other via ClusterIP services.**
* **External users access apps via LoadBalancer (or Ingress on top of ClusterIP).**

## Configuration Management

Configuration Management (CM) is the **process of automating the setup, management, and maintenance of IT systems** (servers, networks, applications, containers, etc.) so that they remain in a **desired, consistent state**.

Instead of manually logging into servers and installing packages or editing config files, configuration management tools help you:

* **Provision** (create) infrastructure.
* **Configure** software and services.
* **Maintain consistency** across environments (dev, test, prod).
* **Automate repetitive tasks**.
* **Version-control configurations** like code (Infrastructure as Code ‚Äì IaC).

üëâ Example:
If you have **50 servers**, and you need to install Python 3.10 + Nginx with the same configuration on all, instead of doing it manually, you write automation scripts using CM tools like **Ansible, Puppet, or Chef**.


## üîπ Ansible ‚Äì Configuration Management Tool

**Ansible** is one of the most popular open-source configuration management and automation tools.

### ‚ú® Key Features of Ansible

* **Agentless** ‚Üí No need to install a client agent on servers (unlike Puppet or Chef). Uses **SSH** or **WinRM**.
* **Idempotent** ‚Üí Ensures repeated executions bring the system to the same state (won‚Äôt reinstall Nginx if it‚Äôs already there).
* **Simple YAML-based syntax** (Playbooks).
* **Cross-platform** ‚Üí Works on Linux, Windows, Cloud, Containers.
* **Scalable** ‚Üí Can manage from a few servers to thousands.


### üîπ How Ansible Works

1. **Control Node** ‚Üí The machine where Ansible is installed.
2. **Managed Nodes** ‚Üí The servers you want to configure.
3. **Inventory File** ‚Üí List of target servers (IPs/hostnames).
4. **Modules** ‚Üí Small programs that Ansible executes on target machines (e.g., for installing packages, copying files, restarting services).
5. **Playbooks** ‚Üí YAML files that define automation tasks.


### üîπ Example: Ansible Playbook

Suppose you want to install **Nginx** on multiple servers:

```yaml
---
- name: Install and start Nginx
  hosts: webservers
  become: yes

  tasks:
    - name: Install Nginx
      apt:
        name: nginx
        state: present
        update_cache: yes

    - name: Start Nginx
      service:
        name: nginx
        state: started
        enabled: yes
```

üëâ Explanation:

* `hosts: webservers` ‚Üí Target group from inventory file.
* `apt` module ‚Üí Installs Nginx.
* `service` module ‚Üí Ensures Nginx is running and enabled on boot.


### üîπ Use Cases of Ansible

* **Configuration Management** (installing packages, setting up users, managing configs).
* **Provisioning** (cloud resources on AWS, Azure, GCP).
* **Application Deployment** (CI/CD pipelines).
* **Orchestration** (managing multiple services across environments).
* **Security & Compliance** (patching, auditing).


‚ö° In short:

* **Configuration Management** ensures consistency of systems.
* **Ansible** is a powerful, agentless tool for automating CM and orchestration using YAML playbooks.

---